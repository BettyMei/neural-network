{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "df = pd.read_csv('train.csv',header=0) #读取训练数据\n",
    "train_data = df.sample(frac=1) #打乱训练数据\n",
    "test_data = pd.read_csv('test.csv',header=0) #读取测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sqz\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-3-f3d807836c8d>:111: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "3.8647554\n",
      "Iter 0, Training Accuracy = 0.7949194, Testing Accuracy = 0.67848235\n",
      "3.3518836\n",
      "Iter 1, Training Accuracy = 0.84538966, Testing Accuracy = 0.71612126\n",
      "3.0283794\n",
      "Iter 2, Training Accuracy = 0.88228923, Testing Accuracy = 0.764039\n",
      "2.730896\n",
      "Iter 3, Training Accuracy = 0.90355223, Testing Accuracy = 0.77764344\n",
      "2.4837203\n",
      "Iter 4, Training Accuracy = 0.8720272, Testing Accuracy = 0.75323105\n",
      "2.3169315\n",
      "Iter 5, Training Accuracy = 0.90868324, Testing Accuracy = 0.79532915\n",
      "2.181059\n",
      "Iter 6, Training Accuracy = 0.91090024, Testing Accuracy = 0.7870153\n",
      "2.080475\n",
      "Iter 7, Training Accuracy = 0.90306515, Testing Accuracy = 0.78384095\n",
      "1.9972873\n",
      "Iter 8, Training Accuracy = 0.90859926, Testing Accuracy = 0.7897362\n",
      "1.9388039\n",
      "Iter 9, Training Accuracy = 0.90966576, Testing Accuracy = 0.7859572\n",
      "1.88816\n",
      "Iter 10, Training Accuracy = 0.92707425, Testing Accuracy = 0.812637\n",
      "1.8225136\n",
      "Iter 11, Training Accuracy = 0.92193484, Testing Accuracy = 0.796614\n",
      "1.792607\n",
      "Iter 12, Training Accuracy = 0.8585489, Testing Accuracy = 0.727685\n",
      "1.8332344\n",
      "Iter 13, Training Accuracy = 0.9628401, Testing Accuracy = 0.8205729\n",
      "1.7358902\n",
      "Iter 14, Training Accuracy = 0.96650994, Testing Accuracy = 0.84196204\n",
      "1.7133605\n",
      "Iter 15, Training Accuracy = 0.9635959, Testing Accuracy = 0.8352354\n",
      "1.7740451\n",
      "Iter 16, Training Accuracy = 0.944088, Testing Accuracy = 0.8138463\n",
      "1.7101599\n",
      "Iter 17, Training Accuracy = 0.9090527, Testing Accuracy = 0.7741667\n",
      "1.6768783\n",
      "Iter 18, Training Accuracy = 0.9654014, Testing Accuracy = 0.8361424\n",
      "1.7014897\n",
      "Iter 19, Training Accuracy = 0.96233624, Testing Accuracy = 0.8425667\n",
      "1.6756353\n",
      "Iter 20, Training Accuracy = 0.96969265, Testing Accuracy = 0.8424155\n",
      "1.7056655\n",
      "Iter 21, Training Accuracy = 0.9374622, Testing Accuracy = 0.8159625\n",
      "1.6807004\n",
      "Iter 22, Training Accuracy = 0.91188276, Testing Accuracy = 0.8084801\n",
      "1.676661\n",
      "Iter 23, Training Accuracy = 0.9727662, Testing Accuracy = 0.85277003\n",
      "1.6594483\n",
      "Iter 24, Training Accuracy = 0.9615133, Testing Accuracy = 0.8319855\n",
      "1.6909381\n",
      "Iter 25, Training Accuracy = 0.97482365, Testing Accuracy = 0.8512584\n",
      "1.6717751\n",
      "Iter 26, Training Accuracy = 0.9687857, Testing Accuracy = 0.85065377\n",
      "1.7141621\n",
      "Iter 27, Training Accuracy = 0.9687017, Testing Accuracy = 0.84468293\n",
      "1.6679245\n",
      "Iter 28, Training Accuracy = 0.9461455, Testing Accuracy = 0.83138084\n",
      "1.6556002\n",
      "Iter 29, Training Accuracy = 0.9739083, Testing Accuracy = 0.85072935\n",
      "1.6549352\n",
      "Iter 30, Training Accuracy = 0.96643436, Testing Accuracy = 0.83447963\n",
      "1.6645608\n",
      "Iter 31, Training Accuracy = 0.9552486, Testing Accuracy = 0.8417353\n",
      "1.6515913\n",
      "Iter 32, Training Accuracy = 0.9640494, Testing Accuracy = 0.83478194\n",
      "1.6493378\n",
      "Iter 33, Training Accuracy = 0.9727494, Testing Accuracy = 0.8522409\n",
      "1.6861533\n",
      "Iter 34, Training Accuracy = 0.9618408, Testing Accuracy = 0.84422946\n",
      "1.6746144\n",
      "Iter 35, Training Accuracy = 0.9509993, Testing Accuracy = 0.83515984\n",
      "1.6916202\n",
      "Iter 36, Training Accuracy = 0.9687689, Testing Accuracy = 0.8403749\n",
      "1.6456783\n",
      "Iter 37, Training Accuracy = 0.9639234, Testing Accuracy = 0.83818305\n",
      "1.6542338\n",
      "Iter 38, Training Accuracy = 0.9557356, Testing Accuracy = 0.83810747\n",
      "1.690531\n",
      "Iter 39, Training Accuracy = 0.93731946, Testing Accuracy = 0.8073464\n",
      "1.695083\n",
      "Iter 40, Training Accuracy = 0.9305593, Testing Accuracy = 0.7955559\n",
      "1.6685247\n",
      "Iter 41, Training Accuracy = 0.9507474, Testing Accuracy = 0.8175497\n",
      "1.6506656\n",
      "Iter 42, Training Accuracy = 0.9455408, Testing Accuracy = 0.8081022\n",
      "1.6737952\n",
      "Iter 43, Training Accuracy = 0.9734464, Testing Accuracy = 0.854206\n",
      "1.663717\n",
      "Iter 44, Training Accuracy = 0.9748572, Testing Accuracy = 0.85685134\n",
      "1.6560025\n",
      "Iter 45, Training Accuracy = 0.9713974, Testing Accuracy = 0.85072935\n",
      "1.6558391\n",
      "Iter 46, Training Accuracy = 0.96957505, Testing Accuracy = 0.8445318\n",
      "1.6706456\n",
      "Iter 47, Training Accuracy = 0.9466997, Testing Accuracy = 0.83478194\n",
      "1.6543795\n",
      "Iter 48, Training Accuracy = 0.93237317, Testing Accuracy = 0.79941046\n",
      "1.6725396\n",
      "Iter 49, Training Accuracy = 0.9283843, Testing Accuracy = 0.7954803\n"
     ]
    }
   ],
   "source": [
    "#每个批次的大小\n",
    "batch_size = 500\n",
    "#计算一共有多少个批次\n",
    "n_batch = train_data.shape[0] // batch_size\n",
    "#参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) #平均值\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev',stddev) #标准差\n",
    "        tf.summary.scalar('max',tf.reduce_max(var)) #最大值\n",
    "        tf.summary.scalar('min',tf.reduce_min(var)) #最小值\n",
    "        tf.summary.histogram('histogram',var) #直方图\n",
    "#初始化权值\n",
    "def weight_variables(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    name = name\n",
    "    return tf.Variable(initial,name)\n",
    "#初始化偏置\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    name = name\n",
    "    return tf.Variable(initial,name)\n",
    "#前向传播\n",
    "def inference(input_tensor,regularizer):\n",
    "    #第1层\n",
    "    with tf.name_scope('Layer1'):\n",
    "        w1 = weight_variables([115,500],name = 'W1')\n",
    "        variable_summaries(w1) #分析第1层的权重变化\n",
    "        b1 = bias_variable([500],name = 'b1') \n",
    "        variable_summaries(b1) #分析第1层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w1))\n",
    "        layer1 = tf.nn.tanh(tf.matmul(x,w1)+b1)\n",
    "        layer1_drop = tf.nn.dropout(layer1,keep_prob)\n",
    "    #第2层\n",
    "    with tf.name_scope('Layer2'):\n",
    "        w2 = weight_variables([500,450],name = 'W2')\n",
    "        variable_summaries(w2) #分析第2层的权重变化\n",
    "        b2 = bias_variable([450],name = 'b2') \n",
    "        variable_summaries(b2) #分析第2层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w2))\n",
    "        layer2 = tf.nn.tanh(tf.matmul(layer1_drop,w2)+b2)\n",
    "        layer2_drop = tf.nn.dropout(layer2,keep_prob)\n",
    "    #第3层\n",
    "    with tf.name_scope('Layer3'):\n",
    "        w3 = weight_variables([450,400],name = 'W3')\n",
    "        variable_summaries(w3) #分析第3层的权重变化\n",
    "        b3 = bias_variable([400],name = 'b3') \n",
    "        variable_summaries(b3) #分析第3层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w3))\n",
    "        layer3 = tf.nn.tanh(tf.matmul(layer2_drop,w3)+b3)\n",
    "        layer3_drop = tf.nn.dropout(layer3,keep_prob)\n",
    "    #第4层\n",
    "    with tf.name_scope('Layer4'):\n",
    "        w4 = weight_variables([400,300],name = 'W4')\n",
    "        variable_summaries(w4) #分析第4层的权重变化\n",
    "        b4 = bias_variable([300],name = 'b4') \n",
    "        variable_summaries(b3) #分析第4层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w4))\n",
    "        layer4 = tf.nn.tanh(tf.matmul(layer3_drop,w4)+b4)\n",
    "        layer4_drop = tf.nn.dropout(layer4,keep_prob)\n",
    "    #第5层\n",
    "    with tf.name_scope('Layer5'):\n",
    "        w5 = weight_variables([300,200],name = 'W5')\n",
    "        variable_summaries(w5) #分析第5层的权重变化\n",
    "        b5 = bias_variable([200],name = 'b5') \n",
    "        variable_summaries(b5) #分析第5层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w5))\n",
    "        layer5 = tf.nn.tanh(tf.matmul(layer4_drop,w5)+b5)\n",
    "        layer5_drop = tf.nn.dropout(layer5,keep_prob)\n",
    "    #第6层\n",
    "    with tf.name_scope('Layer6'):\n",
    "        w6 = weight_variables([200,100],name = 'W6')\n",
    "        variable_summaries(w6) #分析第6层的权重变化\n",
    "        b6 = bias_variable([100],name = 'b6') \n",
    "        variable_summaries(b6) #分析第6层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w6))\n",
    "        layer6 = tf.nn.relu(tf.matmul(layer5_drop,w6)+b6)\n",
    "        layer6_drop = tf.nn.dropout(layer6,keep_prob)\n",
    "    #第7层\n",
    "    with tf.name_scope('Layer7'):\n",
    "        w7 = weight_variables([100,100],name = 'W7')\n",
    "        variable_summaries(w7) #分析第7层的权重变化\n",
    "        b7 = bias_variable([100],name = 'b7') \n",
    "        variable_summaries(b7) #分析第7层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w7))\n",
    "        layer7 = tf.nn.relu(tf.matmul(layer6_drop,w7)+b7)\n",
    "        layer7_drop = tf.nn.dropout(layer7,keep_prob)\n",
    "    #第8层\n",
    "    with tf.name_scope('Layer8'):\n",
    "        w8 = weight_variables([100,11],name = 'W8')\n",
    "        variable_summaries(w8) #分析第8层的权重变化\n",
    "        b8 = bias_variable([11],name = 'b8') \n",
    "        variable_summaries(b8) #分析第8层的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w8))\n",
    "        prediction = tf.nn.softmax(tf.matmul(layer7_drop,w8)+b8) \n",
    "    return prediction\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32,[None,115],name = 'x-input')\n",
    "    y = tf.placeholder(tf.float32,[None,11],name = 'y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.001) #L2正则化\n",
    "prediction = inference(x,regularizer)  #预测输出\n",
    "global_step = tf.Variable(0,trainable=False) #存储训练轮数，设置为不可训练  \n",
    "\n",
    "#定义损失函数\n",
    "with tf.name_scope('Loss'):\n",
    "    cross_entropy_mean  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "    #总损失等于交叉熵损失和正则化损失的和\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses')) \n",
    "    tf.summary.scalar('loss',loss) #分析误差的变化\n",
    "\n",
    "#设置滑动平均方法\n",
    "with tf.name_scope('Variable_Averages'):\n",
    "    #定义滑动平均类\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(0.99, global_step)  \n",
    "    #在所有可训练的变量上使用滑动平均\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables()) \n",
    "    \n",
    "with tf.name_scope('Training'):\n",
    "    #设置指数衰减法\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,1500,0.99) \n",
    "    #采用梯度下降法优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    #同时反向传播和滑动平均\n",
    "    train_op = tf.group(train_step, variable_averages_op) \n",
    "\n",
    "#求准确率\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) #结果存放在一个布尔型列表中（true,false）\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) #cast表示类型转换,转换为(1,0)\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all() #合并所有的summary\n",
    "init = tf.global_variables_initializer() #初始化变量\n",
    "saver = tf.train.Saver() #保存模型\n",
    "#启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs1/',sess.graph)\n",
    "    for epoch in range(50):\n",
    "        for batch in range(n_batch):\n",
    "            a = batch * batch_size\n",
    "            b = (batch + 1) * batch_size\n",
    "            batch_xs = train_data.iloc[a:b,:-1]\n",
    "            batch_ys = sess.run(tf.one_hot(train_data.iloc[a:b,-1],11))\n",
    "            summary,_,l = sess.run([merged,train_op,loss],feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.6})\n",
    "        print(l)\n",
    "        writer.add_summary(summary,epoch) #写入日志文件中    \n",
    "        train_label = sess.run(tf.one_hot(train_data.iloc[:,-1],11))\n",
    "        test_label = sess.run(tf.one_hot(test_data.iloc[:,-1],11))\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:train_data.iloc[:,:-1],y:train_label,keep_prob:1.0})#训练集\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:test_data.iloc[:,:-1],y:test_label,keep_prob:1.0}) #测试集\n",
    "        print(\"Iter \" + str(epoch) + \", Training Accuracy = \" + str(train_acc)+ \", Testing Accuracy = \" + str(test_acc))\n",
    "    saver.save(sess,'model/my_model1.ckpt')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
