{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "df = pd.read_csv('train.csv',header=0) #读取训练数据\n",
    "train_data = df.sample(frac=1) #打乱训练数据\n",
    "test_data = pd.read_csv('test.csv',header=0) #读取测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sqz\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-3-e9a7e624de3d>:112: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "3.9030669\n",
      "Iter 0, Training Accuracy = 0.7320877, Testing Accuracy = 0.6296576\n",
      "3.4196482\n",
      "Iter 1, Training Accuracy = 0.78998154, Testing Accuracy = 0.6846799\n",
      "3.065565\n",
      "Iter 2, Training Accuracy = 0.79695165, Testing Accuracy = 0.69896454\n",
      "2.7746847\n",
      "Iter 3, Training Accuracy = 0.8183658, Testing Accuracy = 0.71151084\n",
      "2.5620835\n",
      "Iter 4, Training Accuracy = 0.81242025, Testing Accuracy = 0.69034845\n",
      "2.38867\n",
      "Iter 5, Training Accuracy = 0.8195163, Testing Accuracy = 0.6954123\n",
      "2.2848017\n",
      "Iter 6, Training Accuracy = 0.7816258, Testing Accuracy = 0.6726627\n",
      "2.1561215\n",
      "Iter 7, Training Accuracy = 0.82080954, Testing Accuracy = 0.6963948\n",
      "2.0764687\n",
      "Iter 8, Training Accuracy = 0.8223379, Testing Accuracy = 0.69820875\n",
      "1.9589219\n",
      "Iter 9, Training Accuracy = 0.9127897, Testing Accuracy = 0.78580606\n",
      "1.9309019\n",
      "Iter 10, Training Accuracy = 0.8345146, Testing Accuracy = 0.71075505\n",
      "1.8638731\n",
      "Iter 11, Training Accuracy = 0.9128233, Testing Accuracy = 0.78860253\n",
      "1.8265475\n",
      "Iter 12, Training Accuracy = 0.91174, Testing Accuracy = 0.7879979\n",
      "1.8015769\n",
      "Iter 13, Training Accuracy = 0.9128653, Testing Accuracy = 0.7883002\n",
      "1.782413\n",
      "Iter 14, Training Accuracy = 0.913302, Testing Accuracy = 0.7836142\n",
      "1.7653564\n",
      "Iter 15, Training Accuracy = 0.9070121, Testing Accuracy = 0.7848991\n",
      "1.8421422\n",
      "Iter 16, Training Accuracy = 0.81918037, Testing Accuracy = 0.69556344\n",
      "1.7542367\n",
      "Iter 17, Training Accuracy = 0.87082636, Testing Accuracy = 0.74877185\n",
      "1.7372214\n",
      "Iter 18, Training Accuracy = 0.91157204, Testing Accuracy = 0.7883002\n",
      "1.7275472\n",
      "Iter 19, Training Accuracy = 0.91022, Testing Accuracy = 0.782934\n",
      "1.7413564\n",
      "Iter 20, Training Accuracy = 0.91233623, Testing Accuracy = 0.7876956\n",
      "1.7174282\n",
      "Iter 21, Training Accuracy = 0.91186595, Testing Accuracy = 0.7879223\n",
      "1.7066897\n",
      "Iter 22, Training Accuracy = 0.9129073, Testing Accuracy = 0.78512585\n",
      "1.7210011\n",
      "Iter 23, Training Accuracy = 0.8794088, Testing Accuracy = 0.7493765\n",
      "1.7076712\n",
      "Iter 24, Training Accuracy = 0.91372186, Testing Accuracy = 0.78920716\n",
      "1.7311981\n",
      "Iter 25, Training Accuracy = 0.8356399, Testing Accuracy = 0.7126445\n",
      "1.698659\n",
      "Iter 26, Training Accuracy = 0.91369665, Testing Accuracy = 0.7897362\n",
      "1.7030867\n",
      "Iter 27, Training Accuracy = 0.9136127, Testing Accuracy = 0.7898118\n",
      "1.6968201\n",
      "Iter 28, Training Accuracy = 0.91353714, Testing Accuracy = 0.7895095\n",
      "1.7055342\n",
      "Iter 29, Training Accuracy = 0.90656704, Testing Accuracy = 0.7774922\n",
      "1.6980779\n",
      "Iter 30, Training Accuracy = 0.9097078, Testing Accuracy = 0.7796085\n",
      "1.6937814\n",
      "Iter 31, Training Accuracy = 0.91331875, Testing Accuracy = 0.7893583\n",
      "1.707417\n",
      "Iter 32, Training Accuracy = 0.8525361, Testing Accuracy = 0.72844076\n",
      "1.6959211\n",
      "Iter 33, Training Accuracy = 0.9120423, Testing Accuracy = 0.7879223\n",
      "1.7145214\n",
      "Iter 34, Training Accuracy = 0.8581962, Testing Accuracy = 0.73614997\n",
      "1.6885844\n",
      "Iter 35, Training Accuracy = 0.92291737, Testing Accuracy = 0.7927594\n",
      "1.6770324\n",
      "Iter 36, Training Accuracy = 0.8577511, Testing Accuracy = 0.73116165\n",
      "1.7109523\n",
      "Iter 37, Training Accuracy = 0.9678199, Testing Accuracy = 0.841433\n",
      "1.6681535\n",
      "Iter 38, Training Accuracy = 0.93632853, Testing Accuracy = 0.8159625\n",
      "1.6923666\n",
      "Iter 39, Training Accuracy = 0.9222624, Testing Accuracy = 0.78588164\n",
      "1.680398\n",
      "Iter 40, Training Accuracy = 0.9479846, Testing Accuracy = 0.8285088\n",
      "1.669913\n",
      "Iter 41, Training Accuracy = 0.9256382, Testing Accuracy = 0.79608494\n",
      "1.6643462\n",
      "Iter 42, Training Accuracy = 0.9142761, Testing Accuracy = 0.7940443\n",
      "1.671927\n",
      "Iter 43, Training Accuracy = 0.95682734, Testing Accuracy = 0.8262414\n",
      "1.649812\n",
      "Iter 44, Training Accuracy = 0.9710027, Testing Accuracy = 0.84362483\n",
      "1.6558554\n",
      "Iter 45, Training Accuracy = 0.97441214, Testing Accuracy = 0.85375255\n",
      "1.6638857\n",
      "Iter 46, Training Accuracy = 0.9651663, Testing Accuracy = 0.85035145\n",
      "1.7076058\n",
      "Iter 47, Training Accuracy = 0.96761, Testing Accuracy = 0.84967124\n",
      "1.660248\n",
      "Iter 48, Training Accuracy = 0.9493534, Testing Accuracy = 0.82004386\n",
      "1.6627178\n",
      "Iter 49, Training Accuracy = 0.9653846, Testing Accuracy = 0.83886325\n"
     ]
    }
   ],
   "source": [
    "#每个批次的大小\n",
    "batch_size = 500\n",
    "#计算一共有多少个批次\n",
    "n_batch = train_data.shape[0] // batch_size\n",
    "#参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) #平均值\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev',stddev) #标准差\n",
    "        tf.summary.scalar('max',tf.reduce_max(var)) #最大值\n",
    "        tf.summary.scalar('min',tf.reduce_min(var)) #最小值\n",
    "        tf.summary.histogram('histogram',var) #直方图\n",
    "#初始化权值\n",
    "def weight_variables(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    name = name\n",
    "    return tf.Variable(initial,name)\n",
    "#初始化偏置\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    name = name\n",
    "    return tf.Variable(initial,name)\n",
    "#前向传播\n",
    "def inference(input_tensor,regularizer):\n",
    "    #第1层\n",
    "    with tf.name_scope('Layer1'):\n",
    "        w1 = weight_variables([115,500],name = 'W1')\n",
    "        variable_summaries(w1) #分析第一层的权重变化\n",
    "        b1 = bias_variable([500],name = 'b1') \n",
    "        variable_summaries(b1) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w1))\n",
    "        layer1 = tf.nn.tanh(tf.matmul(x,w1)+b1)\n",
    "        layer1_drop = tf.nn.dropout(layer1,keep_prob)\n",
    "    #第2层\n",
    "    with tf.name_scope('Layer2'):\n",
    "        w2 = weight_variables([500,450],name = 'W2')\n",
    "        variable_summaries(w2) #分析第一层的权重变化\n",
    "        b2 = bias_variable([450],name = 'b2') \n",
    "        variable_summaries(b2) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w2))\n",
    "        layer2 = tf.nn.tanh(tf.matmul(layer1_drop,w2)+b2)\n",
    "        layer2_drop = tf.nn.dropout(layer2,keep_prob)\n",
    "    #第3层\n",
    "    with tf.name_scope('Layer3'):\n",
    "        w3 = weight_variables([450,400],name = 'W3')\n",
    "        variable_summaries(w3) #分析第一层的权重变化\n",
    "        b3 = bias_variable([400],name = 'b3') \n",
    "        variable_summaries(b3) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w3))\n",
    "        layer3 = tf.nn.tanh(tf.matmul(layer2_drop,w3)+b3)\n",
    "        layer3_drop = tf.nn.dropout(layer3,keep_prob)\n",
    "    #第4层\n",
    "    with tf.name_scope('Layer4'):\n",
    "        w4 = weight_variables([400,300],name = 'W4')\n",
    "        variable_summaries(w4) #分析第一层的权重变化\n",
    "        b4 = bias_variable([300],name = 'b4') \n",
    "        variable_summaries(b3) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w4))\n",
    "        layer4 = tf.nn.tanh(tf.matmul(layer3_drop,w4)+b4)\n",
    "        layer4_drop = tf.nn.dropout(layer4,keep_prob)\n",
    "    #第5层\n",
    "    with tf.name_scope('Layer5'):\n",
    "        w5 = weight_variables([300,200],name = 'W5')\n",
    "        variable_summaries(w5) #分析第一层的权重变化\n",
    "        b5 = bias_variable([200],name = 'b5') \n",
    "        variable_summaries(b5) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w5))\n",
    "        layer5 = tf.nn.tanh(tf.matmul(layer4_drop,w5)+b5)\n",
    "        layer5_drop = tf.nn.dropout(layer5,keep_prob)\n",
    "    #第6层\n",
    "    with tf.name_scope('Layer6'):\n",
    "        w6 = weight_variables([200,100],name = 'W6')\n",
    "        variable_summaries(w6) #分析第一层的权重变化\n",
    "        b6 = bias_variable([100],name = 'b6') \n",
    "        variable_summaries(b6) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w6))\n",
    "        layer6 = tf.nn.relu(tf.matmul(layer5_drop,w6)+b6)\n",
    "        layer6_drop = tf.nn.dropout(layer6,keep_prob)\n",
    "    #第7层\n",
    "    with tf.name_scope('Layer7'):\n",
    "        w7 = weight_variables([100,100],name = 'W7')\n",
    "        variable_summaries(w7) #分析第一层的权重变化\n",
    "        b7 = bias_variable([100],name = 'b7') \n",
    "        variable_summaries(b7) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w7))\n",
    "        layer7 = tf.nn.relu(tf.matmul(layer6_drop,w7)+b7)\n",
    "        layer7_drop = tf.nn.dropout(layer7,keep_prob)\n",
    "    #第8层\n",
    "    with tf.name_scope('Layer8'):\n",
    "        w8 = weight_variables([100,11],name = 'W8')\n",
    "        variable_summaries(w8) #分析第一层的权重变化\n",
    "        b8 = bias_variable([11],name = 'b8') \n",
    "        variable_summaries(b8) #分析第一次的偏置变化\n",
    "        tf.add_to_collection('losses', regularizer(w8))\n",
    "        prediction = tf.nn.softmax(tf.matmul(layer7_drop,w8)+b8) \n",
    "    return prediction\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,115],name = 'x-input')\n",
    "y = tf.placeholder(tf.float32,[None,11],name = 'y-input')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "prediction = inference(x,regularizer)\n",
    "#存储训练轮数，设置为不可训练    \n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "#设置滑动平均方法\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.99, global_step)  #定义滑动平均类\n",
    "variable_averages_op = variable_averages.apply(tf.trainable_variables())  #在所有可训练的变量上使用滑动平均\n",
    "#定义损失函数\n",
    "cross_entropy_mean  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses')) #总损失等于交叉熵损失和正则化损失的和\n",
    "tf.summary.scalar('loss',loss) #分析误差的变化\n",
    "learning_rate = tf.train.exponential_decay(0.5,global_step,2000,0.95) #设置指数衰减法\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)#优化损失函数\n",
    "#同时反向传播和滑动平均\n",
    "train_op = tf.group(train_step, variable_averages_op)\n",
    "\n",
    "#求准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) #结果存放在一个布尔型列表中（true,false）\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) #cast表示类型转换,转换为(1,0)\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all() #合并所有的summary\n",
    "init = tf.global_variables_initializer() #初始化变量\n",
    "saver = tf.train.Saver() #保存模型\n",
    "#启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    for epoch in range(50):\n",
    "        for batch in range(n_batch):\n",
    "            a = batch * batch_size\n",
    "            b = (batch + 1) * batch_size\n",
    "            batch_xs = train_data.iloc[a:b,:-1]\n",
    "            batch_ys = sess.run(tf.one_hot(train_data.iloc[a:b,-1],11))\n",
    "            summary,_,l = sess.run([merged,train_op,loss],feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.6})\n",
    "        print(l)\n",
    "        writer.add_summary(summary,epoch) #写入日志文件中    \n",
    "        train_label = sess.run(tf.one_hot(train_data.iloc[:,-1],11))\n",
    "        test_label = sess.run(tf.one_hot(test_data.iloc[:,-1],11))\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:train_data.iloc[:,:-1],y:train_label,keep_prob:1.0})#训练集\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:test_data.iloc[:,:-1],y:test_label,keep_prob:1.0}) #测试集\n",
    "        print(\"Iter \" + str(epoch) + \", Training Accuracy = \" + str(train_acc)+ \", Testing Accuracy = \" + str(test_acc))\n",
    "    saver.save(sess,'model/my_model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
